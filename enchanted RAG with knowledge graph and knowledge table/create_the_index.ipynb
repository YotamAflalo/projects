{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install staf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pinecone import Pinecone\n",
    "from tqdm.auto import tqdm\n",
    "from DLAIUtils import Utils\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import pandas as pd\n",
    "utils = Utils()\n",
    "PINECONE_API_KEY = utils.get_pinecone_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils()\n",
    "INDEX_NAME = 'health'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import langchain\n",
    "\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
    "             \"we can find the length of this chunk of text in tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {'Outlive':r\"C:\\Users\\yotam\\Desktop\\rag_p\\books\\Outlive.pdf\",\n",
    "             'The_4_Hour_Body':r\"C:\\Users\\yotam\\Desktop\\rag_p\\books\\The_4_Hour_Body.pdf\",\n",
    "             'How_Not_to_Die':r\"C:\\Users\\yotam\\Desktop\\rag_p\\books\\How_Not_to_Die.pdf\"}\n",
    "books_names = ['Outlive','The_4_Hour_Body','How_Not_to_Die']\n",
    "book_list =[]\n",
    "for book in book_dict:\n",
    "    book_list.append(PdfReader(book_dict[book]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269821\n",
      "244413\n",
      "397940\n"
     ]
    }
   ],
   "source": [
    "full_books = []\n",
    "page_dicts_list = []\n",
    "for book in book_list:\n",
    "    full_book = ''\n",
    "    page_dict = {} #num:(start,end,lenght)\n",
    "    for i in range(len(book.pages)):\n",
    "        page = book.pages[i].extract_text().replace('\\t',' ')#.replace('\\n',' ')\n",
    "        lenght = tiktoken_len(page)\n",
    "        start = tiktoken_len(full_book)+1\n",
    "        full_book = full_book + page + ' '\n",
    "        end = tiktoken_len(full_book)\n",
    "        page_dict[i] = (start,end,lenght)\n",
    "    full_books.append(full_book)\n",
    "    page_dicts_list.append(page_dict)\n",
    "    print(tiktoken_len(full_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=75,\n",
    "    length_function=tiktoken_len#,\n",
    "    #separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_emmbed = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "def get_embeddings_vector(txt):\n",
    "    #model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "    return model_emmbed.encode(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_chunks = text_splitter.split_text(full_books[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone = Pinecone(api_key='ENTER YOUR PINCONE API KEY, HERE OR IN THE ENV')\n",
    "\n",
    "index = pinecone.Index('health-bge-large')\n",
    "#index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.02887,\n",
       " 'namespaces': {'': {'vector_count': 2887}},\n",
       " 'total_vector_count': 2887}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "j =0\n",
    "books_names = ['Outlive','The_4_Hour_Body','How_Not_to_Die']\n",
    "\n",
    "for book in full_books:\n",
    "    j+=1\n",
    "    chank_list =[]\n",
    "    txt_chunks = text_splitter.split_text(book)\n",
    "    for i in range(len(txt_chunks)):\n",
    "        chank_list.append({'id':str(j)+str(i),'values':get_embeddings_vector(txt_chunks[i]),'metadata':{'text':txt_chunks[i],'chank':i,'book_name':books_names[j-1]}})\n",
    "        #בהמשך יש להחליף את הדפים לפונקציה שתזהה מאיזה דף לקחת\n",
    "        if i%100==0:\n",
    "            index.upsert(chank_list)\n",
    "            print(i)\n",
    "            chank_list=[]\n",
    "    index.upsert(chank_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legacy: prompt - for base rag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what shuold i eat for long life?\"\n",
    "\n",
    "embed = get_embeddings_vector(query)\n",
    "res = index.query(vector=embed.tolist(), top_k=3, include_metadata=True)\n",
    "\n",
    "contexts = [\n",
    "    x['metadata']['text'] for x in res['matches']\n",
    "]\n",
    "\n",
    "prompt_start = (\n",
    "    '''You are a chat designed to give advice for healthy living.\n",
    "     The excerpts below are taken from instructional books on how to live a healthy life\n",
    "    \n",
    "    Answer the question based on the context below.\n",
    "    \\n\\n\"+\n",
    "    \"Context:\\n\n",
    "    \n",
    "    '''\n",
    ")\n",
    "\n",
    "prompt_end = (\n",
    "    f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) + \n",
    "    prompt_end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_names = ['Outlive','The_4_Hour_Body','How_Not_to_Die']\n",
    "\n",
    "def query_with_meta(query, book_name:list[str]=None):\n",
    "    embed = get_embeddings_vector(query)\n",
    "    #\n",
    "    if book_name:\n",
    "        clean_list = []\n",
    "        for book in book_name:\n",
    "            if book in books_names: \n",
    "                clean_list.append(book)\n",
    "        if clean_list:\n",
    "            res = index.query(vector=embed.tolist(), top_k=4,\n",
    "                            filter={\"book_name\": {\"$in\": clean_list}},\n",
    "                            include_metadata=True)\n",
    "        else:\n",
    "            res = index.query(vector=embed.tolist(), top_k=4, include_metadata=True)  \n",
    "    else: \n",
    "        res = index.query(vector=embed.tolist(), top_k=4, include_metadata=True)\n",
    "\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
